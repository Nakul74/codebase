{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create langchain docs from folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "directory = '/app/dir_path'\n",
    "\n",
    "def load_docs(directory):\n",
    "  loader = DirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "documents = load_docs(directory)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_docs(documents,chunk_size=1024,chunk_overlap=30):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "\n",
    "docs = split_docs(documents)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create custom langchain docs with metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "d = {'how are you?':'I am fine',\n",
    "     'what is your name?':'My name is smith'}\n",
    "\n",
    "docs = []\n",
    "for question,answer in d.items():\n",
    "    doc =  Document(page_content=question, metadata={\"answer\": answer})\n",
    "    docs.append(doc)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create retriever using chromadb\n",
    "* For more information visit [chroma langchain](https://python.langchain.com/docs/integrations/vectorstores/chroma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "openai_api_key = 'xxx'\n",
    "openai_embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "vectorstore = Chroma.from_documents(documents=docs, embedding=openai_embedding)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 50})\n",
    "# retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\",search_kwargs={\"score_threshold\": .5})\n",
    "# retriever = vectorstore.as_retriever(search_type=\"mmr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create retriever using FAISS\n",
    "* For more information visit [FAISS langchain](https://python.langchain.com/docs/integrations/vectorstores/FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "openai_api_key = 'xxx'\n",
    "openai_embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "vectorstore = FAISS.from_documents(documents=docs, embedding=openai_embedding)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 50})\n",
    "# retriever = vectorstore.as_retriever(search_type=\"similarity_score_threshold\",search_kwargs={\"score_threshold\": .5})\n",
    "# retriever = vectorstore.as_retriever(search_type=\"mmr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save database\n",
    "vectorstore.save_local('faiss_index')\n",
    "\n",
    "#load database\n",
    "openai_api_key = 'xxx'\n",
    "openai_embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", openai_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Faiss vectorstore to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_vectorstore_df(vectorstore):\n",
    "    d = vectorstore.docstore._dict\n",
    "    data_rows = []\n",
    "    for chunk_id,content in d.items():\n",
    "        s1 = {\"chunk_id\": chunk_id,\n",
    "            \"content\": content.page_content.strip()}\n",
    "        s = {**s1,**content.metadata}\n",
    "        data_rows.append(s)\n",
    "        \n",
    "    return pd.DataFrame(data_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Faiss vectorstore delete records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note chunk id you can get from above dataframe\n",
    "chunk_id_list = ['abcd','xxxx']\n",
    "vectorstore.delete(ids=chunk_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Faiss vectorstore add new record docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "d = {'how are you?':'I am fine',\n",
    "     'what is your name?':'My name is smith'}\n",
    "\n",
    "docs = []\n",
    "for question,answer in d.items():\n",
    "    doc =  Document(page_content=question, metadata={\"answer\": answer})\n",
    "    docs.append(doc)\n",
    "    \n",
    "vectorstore.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once you have done update and delete you can \n",
    "# save your database\n",
    "vectorstore.save_local('faiss_index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding top k similar docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how many awards did messi won?'\n",
    "similar_docs = retriever.get_relevant_documents(query)\n",
    "similar_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using retrievers as chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "openai_api_key = 'xxx'\n",
    "openai_embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "vectorstore = FAISS.from_documents(documents=docs, embedding=openai_embedding)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "openai_api_key = 'xxx'\n",
    "model_name = 'gpt-3.5-turbo-16k'\n",
    "# check all available models at https://platform.openai.com/docs/models\n",
    "# check pricing at https://openai.com/pricing\n",
    "llm = ChatOpenAI(model_name=model_name,temperature=0.0, openai_api_key=openai_api_key)\n",
    "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory,verbose=True)\n",
    "\n",
    "qa.run({'question':'tell me about goglocal?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom langchain response generation using retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "openai_api_key = 'xxx'\n",
    "model_name = 'gpt-3.5-turbo-16k'\n",
    "# check all available models at https://platform.openai.com/docs/models\n",
    "# check pricing at https://openai.com/pricing\n",
    "chat_model = ChatOpenAI(temperature=0.0, model_name=model_name, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = []\n",
    "response_schemas.append(ResponseSchema(name=\"email_subject\", description=\"subject of email based on context\"))\n",
    "response_schemas.append(ResponseSchema(name=\"email_body\", description=\"body of email based on context\"))\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "            \"You are a marketing specialist at FoodForGood. Your responsibility is to respond to the sender's \"\n",
    "            \"email using the company context provided below. Ensure that your reply is professional and \"\n",
    "            \"incorporates the specified end template for concluding the email response. The sender's email, \"\n",
    "            \"company context, and end template are provided, so be sure to utilize them to craft a professional \"\n",
    "            \"subject and body for the email.It is crucial to precisely respond to each inquiry in the email by \"\n",
    "            \"leveraging the company context provided below.\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = '\\n\\nSender\\'s Email:\\\"\\\"\\\"\\n{input_email}\\n\\\"\\\"\\\"\\n'\n",
    "user_query += '##'*30\n",
    "user_query += '\\n\\nEnd Template:\\n{end_template}\\n\\n'\n",
    "user_query += '##'*30\n",
    "user_query += '\\n\\nCompany Context:\\n{context}\\n\\n'\n",
    "user_query += '##'*30\n",
    "user_query += \"\\n\\n{format_instructions}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variables = []\n",
    "input_variables.append(\"input_email\")\n",
    "input_variables.append(\"context\")\n",
    "input_variables.append(\"end_template\")\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "        HumanMessagePromptTemplate.from_template(user_query)  \n",
    "    ],\n",
    "    input_variables=input_variables,\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain==0.0.345\n",
    "from langchain_core.runnables import (\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    "    RunnableLambda\n",
    ")\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "def extract_docs(input_email):\n",
    "    r = retriever.get_relevant_documents(input_email)\n",
    "    r = ['Q.' + i.page_content.strip() + '\\nA.' + i.metadata['answer'].strip() for i in r]\n",
    "    # r = [f'Q.What vendors are in close proximity, and what menu choices do they offer?\\nA.{s}'] + r\n",
    "    return r\n",
    "\n",
    "get_docs = RunnableLambda(extract_docs)\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"context\": itemgetter(\"input_email\") | get_docs, \n",
    "        \"input_email\": itemgetter(\"input_email\"),\n",
    "        \"end_template\": itemgetter(\"end_template\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = retrieval | prompt | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_email = \"\"\"Hello I’m from Anna McCrea Public School and i want to know which are nearby vendors and what menu options you provide?\"\"\"\n",
    "\n",
    "end_template = \"\"\"Warmest regards,\n",
    "Customer Service Team\n",
    "support@foodforgood.ca\"\"\"\n",
    "\n",
    "d = {'input_email' : full_email,\n",
    "     'end_template' : end_template}\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_response = chain.invoke(d)\n",
    "\n",
    "print(parse_response['email_subject'])\n",
    "print(parse_response['email_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check prompt\n",
    "model_input_prompt = (retrieval | prompt).invoke(d)\n",
    "print(model_input_prompt.messages[0].content)\n",
    "print(model_input_prompt.messages[1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate token length for gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade tiktoken\n",
    "#pip install --upgrade openai\n",
    "\n",
    "import tiktoken\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for openai\n",
    "num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate token length for huggingface models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huggingface_model_token_len(tokenizer,prompt):\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "    return model_inputs, len(model_inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "prompt = \"My favourite condiment is\"\n",
    "model_inputs, token_len = huggingface_model_token_len(tokenizer,prompt)\n",
    "\n",
    "print(token_len)\n",
    "\n",
    "model_inputs.to(device)\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n",
    "response = tokenizer.batch_decode(generated_ids)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from tqdm.auto import tqdm\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')\n",
    "\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "pinecone_region = os.environ.get(\"PINECONE_REGION\")\n",
    "pinecone_vector_db_name = os.environ.get(\"PINECONE_VECTOR_DB\")\n",
    "\n",
    "openai_embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "pinecone.init(api_key=pinecone_api_key, environment=pinecone_region)\n",
    "index = pinecone.Index(pinecone_vector_db_name)\n",
    "\n",
    "df = pd.read_csv('file.csv')\n",
    "\n",
    "vector = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    query_vec = openai_embedding.embed_query(row.descp)\n",
    "    vector.append({'id':uuid.uuid4().hex, \n",
    "          'values':query_vec, \n",
    "          'metadata':{'hs_code': row.hs_code,'parent':row.parent,'country':row.country,'descp':row.descp}})\n",
    "    if len(vector) == 50:\n",
    "        index.upsert(vectors=vector)\n",
    "        vector = []\n",
    "index.upsert(vectors=vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim_hsn(query,hsn,country,k=3):\n",
    "    query_vec = openai_embedding.embed_query(query)\n",
    "    query_response = index.query(\n",
    "        top_k=k,\n",
    "        include_metadata=True,\n",
    "        vector=query_vec,\n",
    "        filter={'country': country,'parent': hsn})\n",
    "    \n",
    "    return query_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
