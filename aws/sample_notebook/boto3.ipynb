{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "def get_aws_client(service,access_key,secret_key,region):\n",
    "    s3_client = boto3.client(service_name = service, \n",
    "                            aws_access_key_id = access_key,\n",
    "                            aws_secret_access_key = secret_key,\n",
    "                            region_name = region)\n",
    "    return s3_client\n",
    "\n",
    "\n",
    "def s3_list_bucket_names(s3_client):\n",
    "    response = s3_client.list_buckets()\n",
    "    buckets = []\n",
    "    for bucket in response['Buckets']:\n",
    "        buckets.append(bucket[\"Name\"])\n",
    "    return buckets\n",
    "\n",
    "def s3_get_data_structure(s3_client,bucket_name):\n",
    "    try:\n",
    "        res = s3_client.list_objects(Bucket = bucket_name)['Contents']\n",
    "    except Exception as e:\n",
    "        return []\n",
    "    paths = []\n",
    "    for i in res:\n",
    "        paths.append(i['Key'])\n",
    "    return paths\n",
    "\n",
    "def s3_create_folder(s3_client,bucket_name,folder_path):\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=(folder_path+'/'))\n",
    "    \n",
    "def s3_check_folder_exists(s3_client,bucket_name,folder_path):\n",
    "    path = path.rstrip('/') \n",
    "    resp = s3_client.list_objects(Bucket=bucket_name, Prefix=folder_path, Delimiter='/',MaxKeys=1)\n",
    "    return 'CommonPrefixes' in resp\n",
    "\n",
    "def upload_file(s3_client,local_file_path, bucket_name, s3_file_path):\n",
    "    s3_client.upload_file(Filename=local_file_path,\n",
    "                          Bucket=bucket_name,\n",
    "                          Key=s3_file_path)\n",
    "    \n",
    "def get_file(s3_client,bucket_name, file_path):\n",
    "    obj = s3_client.get_object(Bucket= bucket_name, Key= file_path) \n",
    "    return obj['Body']\n",
    "\n",
    "def generate_presigned_url(s3_client, bucket_name, object_key, expiration_time=604800):\n",
    "    url = s3_client.generate_presigned_url(\n",
    "        'get_object',\n",
    "        Params={\n",
    "            'Bucket': bucket_name,\n",
    "            'Key': object_key\n",
    "        },\n",
    "        ExpiresIn=expiration_time\n",
    "    )\n",
    "\n",
    "    return url\n",
    "\n",
    "def get_df_from_s3(s3_client, bucket_name, s3_key):\n",
    "    obj = s3_client.get_object(Bucket = bucket_name, Key = s3_key) \n",
    "    return pd.read_csv(obj['Body'])\n",
    "\n",
    "def delete_object(s3_client, bucket_name,s3_key):\n",
    "    s3_client.delete_object(Bucket=bucket_name, Key=s3_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The 's3_key' or 's3_file_path' must begin with the folder inside the specified bucket. For example, if you intend to upload the file 'data.csv' to the bucket 'sample-bucket' within the folder 'sample_data/data/my_data.csv,' follow these steps:\n",
    "1. Call the function 'get_aws_client' to obtain the 's3_client.'\n",
    "2. Utilize the 'upload_file' function with the following parameters:\n",
    "* It's important to note that the full S3 file path, \"s3://sample-bucket/sample_data/data/my_data.csv,\" should not be used directly for the s3_file_path parameter. Instead, only the file path after the bucket name should be specified. The necessary folders in S3 will be automatically created when the file is uploaded to the specified path.\n",
    "    - `local_file_path` set to 'data.csv'\n",
    "    - `bucket_name` set to 'sample-bucket'\n",
    "    - `s3_file_path` set to 'sample_data/data/my_data.csv'\n",
    "\n",
    "*Note:* There is no need to manually create folders in S3; they will be automatically generated once the file is uploaded to the specified path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
